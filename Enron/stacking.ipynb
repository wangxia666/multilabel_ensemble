{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "import random, arff\n",
    "import os, sys, subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = os.listdir(\".\")\n",
    "#number of labels\n",
    "col = list(range(0, 54))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing the files\n",
    "test = pd.read_csv('Enron_a_test1.arff',sep=',',header=None).iloc[:,col]\n",
    "BR   = pd.read_csv('BR-Enron_a_test1.arff',sep=',',header=None).iloc[:,col]\n",
    "CC   = pd.read_csv('CC-Enron_a_test1.arff',sep=',',header=None).iloc[:,col]\n",
    "CDN  = pd.read_csv('CDN-Enron_a_test1.arff',sep=',',header=None).iloc[:,col]\n",
    "CDT  = pd.read_csv('CDT-Enron_a_test1.arff',sep=',',header=None).iloc[:,col]\n",
    "CT   = pd.read_csv('CT-Enron_a_test1.arff',sep=',',header=None).iloc[:,col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    clf_name = (BR,CC,CDN,CDT,CT,test)\n",
    "    for name in clf_name:\n",
    "        name.columns = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_cal(y_true, y_pred, print_all = False):\n",
    "    if(y_true.shape != y_pred.shape):\n",
    "        print(\"Wrong y_preds matrics!\")\n",
    "\n",
    "    real_pos = real_neg = pred_pos = pred_neg  = true_pos = true_neg = []\n",
    "\n",
    "    for i in range(y_true.shape[0]):\n",
    "        # real values - RP and RN\n",
    "        real_pos = np.asarray(np.append(real_pos,np.logical_and(y_true[i], y_true[i]).sum()), dtype=np.int64).reshape(-1,1)\n",
    "        real_neg = np.asarray(np.append(real_neg,np.logical_and(np.logical_not(y_true[i]),np.logical_not(y_true[i])).sum()), dtype=np.int64).reshape(-1,1)\n",
    "\n",
    "        # y_pred values - PP and PN\n",
    "        pred_pos = np.asarray(np.append(pred_pos,np.logical_and(y_pred[i], y_pred[i]).sum()),dtype=np.int64).reshape(-1,1)\n",
    "        pred_neg = np.asarray(np.append(pred_neg,np.logical_and(np.logical_not(y_pred[i]), np.logical_not(y_pred[i])).sum()),dtype=np.int64).reshape(-1,1)\n",
    "\n",
    "        # true labels - TP and TN\n",
    "        true_pos = np.asarray(np.append(true_pos,np.logical_and(y_true[i], y_pred[i]).sum()),dtype=np.int64).reshape(-1,1)\n",
    "        true_neg = np.asarray(np.append(true_neg,np.logical_and(np.logical_not(y_true[i]), np.logical_not(y_pred[i])).sum()),dtype=np.int64).reshape(-1,1)\n",
    "\n",
    "    if print_all:\n",
    "\t\t# if print_all = True - it prints RP, RN, PP, PN, TP and TN\n",
    "        result = np.concatenate((real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg), axis=1)\n",
    "        print(result)\n",
    "\n",
    "    return(real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg)\n",
    "\n",
    "#function to resolve divide by zero error and accept the value 0 when divided by 0\n",
    "def divideZero( value_a, value_b):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        result = np.true_divide( value_a, value_b )\n",
    "        result[ ~ np.isfinite( result )] = 0\n",
    "    return result\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    #return the accuracy - example based\n",
    "    real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg = pre_cal(y_true,y_pred)\n",
    "    score = (true_pos + true_neg)/(pred_pos + pred_neg)\n",
    "    score = np.mean(score)\n",
    "    return score\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    #return precision - example based\n",
    "    real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg = pre_cal(y_true,y_pred)\n",
    "    score = divideZero(true_pos, pred_pos)\n",
    "    score = np.mean(score)\n",
    "    return score\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    #return precision - example based\n",
    "    real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg = pre_cal(y_true,y_pred)\n",
    "    score = divideZero(true_pos, real_pos)\n",
    "    score = np.mean(score)\n",
    "    return score\n",
    "\n",
    "\n",
    "def fscore(y_true, y_pred,beta = 1):\n",
    "\t#return f(beta)score - example based : default beta value is 1\n",
    "    prec, rec = precision(y_true, y_pred), recall(y_true, y_pred)\n",
    "    beta_val = beta*beta\n",
    "    score = ((1+beta_val)*(prec*rec))/(beta_val*(prec+rec))\n",
    "    return score\n",
    "\n",
    "\n",
    "def hamloss(y_true, y_pred):\n",
    "\t#return hamming loss - example based\n",
    "    hamloss = list()\n",
    "    for i in range(y_true.shape[0]):\n",
    "        hamloss = np.asarray(np.append(hamloss,np.logical_xor(y_true[i], y_pred[i]).sum()), dtype=np.int64).reshape(-1,1)\n",
    "    score = (hamloss.sum())/((y_true.shape[0])*(y_true.shape[1]))\n",
    "    return score\n",
    "\n",
    "\n",
    "def subset(y_true, y_pred):\n",
    "\t#return subset accuracy - example based\n",
    "    subset_matrix = list()\n",
    "    for i in range(y_true.shape[0]):\n",
    "        subset_matrix = np.asarray(np.append(subset_matrix, np.array_equal(y_true[i],y_pred[i])), dtype=np.int64).reshape(-1,1)\n",
    "    score = (subset_matrix.sum())/((y_true.shape[0])*(y_true.shape[1]))\n",
    "    return score\n",
    "\n",
    "def zeroloss(y_true, y_pred):\n",
    "    #return new array with removed element having all zero in y_true\n",
    "    condition = list()\n",
    "    index = list()\n",
    "    for i in range(y_true.shape[0]):\n",
    "        new_true = new_pred = list()\n",
    "        condition = np.logical_and(y_true[i],y_true[i]).sum()\n",
    "        if (condition==0):\n",
    "            index = np.asarray(np.append(index,i), dtype = np.int64)\n",
    "\n",
    "        new_true = np.delete(y_true,index, axis = 0)\n",
    "        new_pred = np.delete(y_pred,index, axis = 0)\n",
    "    return new_true, new_pred\n",
    "\n",
    "def microprecision(y_true, y_pred):\n",
    "    #return micro-precision\n",
    "    real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg = pre_cal(y_true,y_pred)\n",
    "    score = true_pos.sum()/pred_pos.sum()\n",
    "    return score\n",
    "\n",
    "def microrecall(y_true, y_pred):\n",
    "    #return micro-recall\n",
    "    real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg = pre_cal(y_true,y_pred)\n",
    "    score = true_pos.sum()/real_pos.sum()\n",
    "    return score\n",
    "\n",
    "def microfscore(y_true, y_pred,beta = 1):\n",
    "    #return micro-fscore\n",
    "    prec, rec = microprecision(y_true, y_pred), microrecall(y_true, y_pred)\n",
    "    beta_val = beta*beta\n",
    "    score = ((1+beta_val)*(prec*rec))/(beta_val*(prec+rec))\n",
    "    return score\n",
    "\n",
    "def macroprecision(y_true, y_pred):\n",
    "    #return macro-precision\n",
    "    real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg = pre_cal(y_true,y_pred)\n",
    "    score = divideZero(true_pos, pred_pos)\n",
    "    return score\n",
    "\n",
    "def macrorecall(y_true, y_pred):\n",
    "    #return macro-recall\n",
    "    real_pos, real_neg, pred_pos, pred_neg, true_pos, true_neg = pre_cal(y_true,y_pred)\n",
    "    score = divideZero(true_pos, real_pos)\n",
    "    return score\n",
    "\n",
    "def macrofscore(y_true, y_pred,beta = 1):\n",
    "    #return macro-fscore\n",
    "    prec, rec = macroprecision(y_true, y_pred), macrorecall(y_true, y_pred)\n",
    "    beta_val = beta*beta\n",
    "    score = divideZero(((1+beta_val)*(prec*rec)),(beta_val*(prec+rec)))\n",
    "    score = np.mean(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_all(np_test, np_pred, output):\n",
    "    value = list()\n",
    "    value.append(accuracy(np_test,np_pred))\n",
    "    value.append(precision(np_test,np_pred))\n",
    "    value.append(recall(np_test,np_pred))\n",
    "    value.append(fscore(np_test,np_pred))\n",
    "    value.append(hamloss(np_test,np_pred))\n",
    "    value.append(subset(np_test,np_pred))\n",
    "    value.append(microfscore(np_test,np_pred))\n",
    "    value.append(macrofscore(np_test,np_pred))\n",
    "    output.append(value)\n",
    "    if False:\n",
    "        print(\"Accuracy : {0:.4f}\".format(value[2]))\n",
    "        print(\"Precision: {0:.4f}\".format(value[3]))\n",
    "        print(\"Recall   : {0:.4f}\".format(value[4]))\n",
    "        print(\"F1-Score : {0:.4f}\".format(value[5]))\n",
    "        print(\"HammingL : {0:.4f}\".format(value[6]))\n",
    "        print(\"Subset   : {0:.4f}\".format(value[7]))\n",
    "        print(\"Micro - F1-Score : {0:.4f}\".format(value[8]))\n",
    "        print(\"Macro - F1-Score : {0:.4f}\".format(value[9]))\n",
    "        print(\"----------------------------------\")\n",
    "    del value\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_name = list()\n",
    "for x in range(len(col)):\n",
    "    label_name.append(pd.DataFrame(pd.concat([BR[x],CC[x],CDN[x],CDT[x],CT[x]], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_name = list()\n",
    "for x in range(len(col)):\n",
    "    test_name.append(test[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state= 42, shuffle= True) \n",
    "\n",
    "def implement_kfold(label_,test_):\n",
    "    output = list()\n",
    "    clf = LogisticRegression(C=1e5)\n",
    "    X = label_.values\n",
    "    y = test_.values\n",
    "    y.resize(len(y))\n",
    "    kf.get_n_splits(X)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        #test-train split for K- Fold \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "   \n",
    "        #train the clasifier\n",
    "        clf.fit(X_train,y_train)\n",
    "        #predict the result\n",
    "        y_pred = clf.predict(X_test)\n",
    "        if False:\n",
    "            print(\"-------------------------------\")\n",
    "            print(\"X_train: {}, y_train: {}\".format(len(X_train),len(y_train)))\n",
    "            print(\"X_test : {}, y_test : {}\".format(len(X_test),len(y_test)))\n",
    "            print(\"y_pred: {}\".format(len(y_pred)))\n",
    "            print(\"-------------------------------\")\n",
    "        y_test.resize(len(y_test),1)\n",
    "        y_pred.resize(len(y_pred),1)\n",
    "        #get the score\n",
    "        calculate_all(y_test,y_pred,output)\n",
    "    del clf\n",
    "    data = pd.DataFrame(output)\n",
    "    return(data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\abhishek verma\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:61: RuntimeWarning: invalid value encountered in double_scalars\n",
      "c:\\users\\abhishek verma\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:99: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "c:\\users\\abhishek verma\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:105: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "c:\\users\\abhishek verma\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:112: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "result = list()\n",
    "for x in range(len(col)):\n",
    "    list_ = [2,8,36,45,47,51,52]\n",
    "    if x in list_:\n",
    "        result.append([0,0,0,0,0,0,0,0])\n",
    "        continue\n",
    "    result.append(implement_kfold(label_name[x],pd.DataFrame(test_name[x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f-score</th>\n",
       "      <th>hamloss</th>\n",
       "      <th>subset</th>\n",
       "      <th>microf</th>\n",
       "      <th>macrof</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.994690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005310</td>\n",
       "      <td>0.994690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.970067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.029933</td>\n",
       "      <td>0.970067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.982379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>0.982379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.943642</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.008811</td>\n",
       "      <td>0.056358</td>\n",
       "      <td>0.943642</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.003524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.947213</td>\n",
       "      <td>0.014097</td>\n",
       "      <td>0.014097</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>0.052787</td>\n",
       "      <td>0.947213</td>\n",
       "      <td>0.404167</td>\n",
       "      <td>0.014097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.765704</td>\n",
       "      <td>0.441764</td>\n",
       "      <td>0.441764</td>\n",
       "      <td>0.441764</td>\n",
       "      <td>0.234296</td>\n",
       "      <td>0.765704</td>\n",
       "      <td>0.789109</td>\n",
       "      <td>0.441764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.977053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022947</td>\n",
       "      <td>0.977053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.985903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014097</td>\n",
       "      <td>0.985903</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.989443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010557</td>\n",
       "      <td>0.989443</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.792346</td>\n",
       "      <td>0.197205</td>\n",
       "      <td>0.197205</td>\n",
       "      <td>0.197205</td>\n",
       "      <td>0.207654</td>\n",
       "      <td>0.792346</td>\n",
       "      <td>0.659324</td>\n",
       "      <td>0.197205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.936625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063375</td>\n",
       "      <td>0.936625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.901382</td>\n",
       "      <td>0.019376</td>\n",
       "      <td>0.019376</td>\n",
       "      <td>0.024220</td>\n",
       "      <td>0.098618</td>\n",
       "      <td>0.901382</td>\n",
       "      <td>0.319538</td>\n",
       "      <td>0.019376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.714687</td>\n",
       "      <td>0.332650</td>\n",
       "      <td>0.332650</td>\n",
       "      <td>0.332650</td>\n",
       "      <td>0.285313</td>\n",
       "      <td>0.714687</td>\n",
       "      <td>0.698074</td>\n",
       "      <td>0.332650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.991181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>0.991181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.996476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.996476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.984164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015836</td>\n",
       "      <td>0.984164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.982394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017606</td>\n",
       "      <td>0.982394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.992951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007049</td>\n",
       "      <td>0.992951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.936625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063375</td>\n",
       "      <td>0.936625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.887238</td>\n",
       "      <td>0.010557</td>\n",
       "      <td>0.010557</td>\n",
       "      <td>0.017596</td>\n",
       "      <td>0.112762</td>\n",
       "      <td>0.887238</td>\n",
       "      <td>0.236451</td>\n",
       "      <td>0.010557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.964773</td>\n",
       "      <td>0.008787</td>\n",
       "      <td>0.008787</td>\n",
       "      <td>0.010984</td>\n",
       "      <td>0.035227</td>\n",
       "      <td>0.964773</td>\n",
       "      <td>0.413690</td>\n",
       "      <td>0.008787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.929607</td>\n",
       "      <td>0.012358</td>\n",
       "      <td>0.012358</td>\n",
       "      <td>0.030896</td>\n",
       "      <td>0.070393</td>\n",
       "      <td>0.929607</td>\n",
       "      <td>0.509615</td>\n",
       "      <td>0.012358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.947167</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.052833</td>\n",
       "      <td>0.947167</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.001770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.667226</td>\n",
       "      <td>0.193603</td>\n",
       "      <td>0.193603</td>\n",
       "      <td>0.193603</td>\n",
       "      <td>0.332774</td>\n",
       "      <td>0.667226</td>\n",
       "      <td>0.533350</td>\n",
       "      <td>0.193603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.987657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012343</td>\n",
       "      <td>0.987657</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.996491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.996491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.991181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>0.991181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.964773</td>\n",
       "      <td>0.051032</td>\n",
       "      <td>0.051032</td>\n",
       "      <td>0.051032</td>\n",
       "      <td>0.035227</td>\n",
       "      <td>0.964773</td>\n",
       "      <td>0.744987</td>\n",
       "      <td>0.051032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.996476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.996476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.963003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.036997</td>\n",
       "      <td>0.963003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.996476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.996476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.996460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.996460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.984179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015821</td>\n",
       "      <td>0.984179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.996476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>0.996476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.978885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021115</td>\n",
       "      <td>0.978885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.987657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012343</td>\n",
       "      <td>0.987657</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.899565</td>\n",
       "      <td>0.061559</td>\n",
       "      <td>0.061559</td>\n",
       "      <td>0.061559</td>\n",
       "      <td>0.100435</td>\n",
       "      <td>0.899565</td>\n",
       "      <td>0.549411</td>\n",
       "      <td>0.061559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.992936</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007064</td>\n",
       "      <td>0.992936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.980671</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019329</td>\n",
       "      <td>0.980671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.975392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024608</td>\n",
       "      <td>0.975392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.941903</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.058097</td>\n",
       "      <td>0.941903</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.001754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.911908</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>0.007018</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.088092</td>\n",
       "      <td>0.911908</td>\n",
       "      <td>0.325397</td>\n",
       "      <td>0.007018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.848610</td>\n",
       "      <td>0.065223</td>\n",
       "      <td>0.065223</td>\n",
       "      <td>0.065223</td>\n",
       "      <td>0.151390</td>\n",
       "      <td>0.848610</td>\n",
       "      <td>0.445174</td>\n",
       "      <td>0.065223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.989427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010573</td>\n",
       "      <td>0.989427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.904906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.095094</td>\n",
       "      <td>0.904906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.987657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012343</td>\n",
       "      <td>0.987657</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  precision    recall   f-score   hamloss    subset    microf  \\\n",
       "0   0.994690   0.000000  0.000000       NaN  0.005310  0.994690       NaN   \n",
       "1   0.970067   0.000000  0.000000       NaN  0.029933  0.970067       NaN   \n",
       "2   0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3   0.982379   0.000000  0.000000       NaN  0.017621  0.982379       NaN   \n",
       "4   0.943642   0.003524  0.003524  0.008811  0.056358  0.943642  0.253968   \n",
       "5   0.947213   0.014097  0.014097  0.017621  0.052787  0.947213  0.404167   \n",
       "6   0.765704   0.441764  0.441764  0.441764  0.234296  0.765704  0.789109   \n",
       "7   0.977053   0.000000  0.000000       NaN  0.022947  0.977053       NaN   \n",
       "8   0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.985903   0.000000  0.000000       NaN  0.014097  0.985903       NaN   \n",
       "10  0.989443   0.000000  0.000000       NaN  0.010557  0.989443       NaN   \n",
       "11  0.792346   0.197205  0.197205  0.197205  0.207654  0.792346  0.659324   \n",
       "12  0.936625   0.000000  0.000000       NaN  0.063375  0.936625       NaN   \n",
       "13  0.901382   0.019376  0.019376  0.024220  0.098618  0.901382  0.319538   \n",
       "14  0.714687   0.332650  0.332650  0.332650  0.285313  0.714687  0.698074   \n",
       "15  0.991181   0.000000  0.000000       NaN  0.008819  0.991181       NaN   \n",
       "16  0.996476   0.000000  0.000000       NaN  0.003524  0.996476       NaN   \n",
       "17  0.984164   0.000000  0.000000       NaN  0.015836  0.984164       NaN   \n",
       "18  0.982394   0.000000  0.000000       NaN  0.017606  0.982394       NaN   \n",
       "19  0.992951   0.000000  0.000000       NaN  0.007049  0.992951       NaN   \n",
       "20  0.936625   0.000000  0.000000       NaN  0.063375  0.936625       NaN   \n",
       "21  0.887238   0.010557  0.010557  0.017596  0.112762  0.887238  0.236451   \n",
       "22  0.964773   0.008787  0.008787  0.010984  0.035227  0.964773  0.413690   \n",
       "23  0.929607   0.012358  0.012358  0.030896  0.070393  0.929607  0.509615   \n",
       "24  0.947167   0.001770  0.001770  0.008850  0.052833  0.947167  0.200000   \n",
       "25  0.667226   0.193603  0.193603  0.193603  0.332774  0.667226  0.533350   \n",
       "26  0.987657   0.000000  0.000000       NaN  0.012343  0.987657       NaN   \n",
       "27  0.996491   0.000000  0.000000       NaN  0.003509  0.996491       NaN   \n",
       "28  0.991181   0.000000  0.000000       NaN  0.008819  0.991181       NaN   \n",
       "29  0.964773   0.051032  0.051032  0.051032  0.035227  0.964773  0.744987   \n",
       "30  0.996476   0.000000  0.000000       NaN  0.003524  0.996476       NaN   \n",
       "31  0.963003   0.000000  0.000000       NaN  0.036997  0.963003       NaN   \n",
       "32  0.996476   0.000000  0.000000       NaN  0.003524  0.996476       NaN   \n",
       "33  0.996460   0.000000  0.000000       NaN  0.003540  0.996460       NaN   \n",
       "34  0.984179   0.000000  0.000000       NaN  0.015821  0.984179       NaN   \n",
       "35  0.996476   0.000000  0.000000       NaN  0.003524  0.996476       NaN   \n",
       "36  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "37  0.978885   0.000000  0.000000       NaN  0.021115  0.978885       NaN   \n",
       "38  0.987657   0.000000  0.000000       NaN  0.012343  0.987657       NaN   \n",
       "39  0.899565   0.061559  0.061559  0.061559  0.100435  0.899565  0.549411   \n",
       "40  0.992936   0.000000  0.000000       NaN  0.007064  0.992936       NaN   \n",
       "41  0.980671   0.000000  0.000000       NaN  0.019329  0.980671       NaN   \n",
       "42  0.975392   0.000000  0.000000       NaN  0.024608  0.975392       NaN   \n",
       "43  0.941903   0.001754  0.001754  0.008772  0.058097  0.941903  0.153846   \n",
       "44  0.911908   0.007018  0.007018  0.017544  0.088092  0.911908  0.325397   \n",
       "45  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "46  0.848610   0.065223  0.065223  0.065223  0.151390  0.848610  0.445174   \n",
       "47  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "48  0.989427   0.000000  0.000000       NaN  0.010573  0.989427       NaN   \n",
       "49  0.904906   0.000000  0.000000       NaN  0.095094  0.904906       NaN   \n",
       "50  0.987657   0.000000  0.000000       NaN  0.012343  0.987657       NaN   \n",
       "51  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "52  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      macrof  \n",
       "0   0.000000  \n",
       "1   0.000000  \n",
       "2   0.000000  \n",
       "3   0.000000  \n",
       "4   0.003524  \n",
       "5   0.014097  \n",
       "6   0.441764  \n",
       "7   0.000000  \n",
       "8   0.000000  \n",
       "9   0.000000  \n",
       "10  0.000000  \n",
       "11  0.197205  \n",
       "12  0.000000  \n",
       "13  0.019376  \n",
       "14  0.332650  \n",
       "15  0.000000  \n",
       "16  0.000000  \n",
       "17  0.000000  \n",
       "18  0.000000  \n",
       "19  0.000000  \n",
       "20  0.000000  \n",
       "21  0.010557  \n",
       "22  0.008787  \n",
       "23  0.012358  \n",
       "24  0.001770  \n",
       "25  0.193603  \n",
       "26  0.000000  \n",
       "27  0.000000  \n",
       "28  0.000000  \n",
       "29  0.051032  \n",
       "30  0.000000  \n",
       "31  0.000000  \n",
       "32  0.000000  \n",
       "33  0.000000  \n",
       "34  0.000000  \n",
       "35  0.000000  \n",
       "36  0.000000  \n",
       "37  0.000000  \n",
       "38  0.000000  \n",
       "39  0.061559  \n",
       "40  0.000000  \n",
       "41  0.000000  \n",
       "42  0.000000  \n",
       "43  0.001754  \n",
       "44  0.007018  \n",
       "45  0.000000  \n",
       "46  0.065223  \n",
       "47  0.000000  \n",
       "48  0.000000  \n",
       "49  0.000000  \n",
       "50  0.000000  \n",
       "51  0.000000  \n",
       "52  0.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column = ['accuracy','precision','recall','f-score','hamloss','subset','microf','macrof']\n",
    "df = pd.DataFrame(np.asarray(result), columns= column)\n",
    "df.head(53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('Stacking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0\n",
      "accuracy   0.823215\n",
      "precision  0.027187\n",
      "recall     0.027187\n",
      "f-score    0.063923\n",
      "hamloss    0.047155\n",
      "subset     0.823215\n",
      "microf     0.343171\n",
      "macrof     0.027187\n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame(df.mean())\n",
    "result.to_csv('Result.csv')\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
